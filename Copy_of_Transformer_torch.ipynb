{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOrufQwo1+ElMU4Ajv2YNw1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sauhardya007/Scrabble_analysis.ipynb/blob/main/Copy_of_Transformer_torch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9r-HYtlMbLr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class InputEmbedding(nn.Module):\n",
        "  def __init__(self, d_model:int,vocab_size:int):\n",
        "    super().__init__()\n",
        "    self.d_model=d_model\n",
        "    self.vocab_size=vocab_size\n",
        "    self.embedding=nn.Embedding(vocab_size,d_model)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.embedding(x) * math.sqrt(self.d_model)\n"
      ],
      "metadata": {
        "id": "mzsWk2EoM0Sr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self,d_model:int,dropout:float,seq_len:int):\n",
        "    super().__init__()\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "    self.d_model=d_model\n",
        "    self.seq_len=seq_len\n",
        "    #create a matrix of (seq_len,d_model)\n",
        "    pe=torch.zeros(seq_len,d_model)\n",
        "    #creating a vector of (Seq_len,1)\n",
        "    position=torch.arange(0,seq_len,dtype=torch.float).unsqueeze(1)\n",
        "    div_term=torch.exp(torch.arange(0,d_model,2).float() * (-math.log(10000.0)/d_model))\n",
        "    #apply sin to even posns\n",
        "    pe[:,0::2]=torch.sin(position * div_term)\n",
        "    #apply cos to odd posns\n",
        "    pe[:,1::2]=torch.cos(position * div_term)\n",
        "    pe=pe.unsqueeze(0)\n",
        "    self.register_buffer('pe',pe)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    x=x+self.pe[:, :x.size(1),:]\n",
        "    return self.dropout(x)\n"
      ],
      "metadata": {
        "id": "0s1rwJgMlU-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self,d_model:int,ffd_hidden:int,num_heads:int,dropout:float):\n",
        "    super().__init__()\n",
        "    self.layer_norm2=nn.LayerNorm(d_model)\n",
        "\n",
        "    #ffd network\n",
        "    self.linear2=nn.Linear(ffd_hidden,d_model)\n",
        "    self.dropout1=nn.Dropout(dropout)\n",
        "    self.activation=nn.ReLU()\n",
        "    self.linear1=nn.Linear(d_model,ffd_hidden)\n",
        "\n",
        "    #Multi-head attention\n",
        "    self.dropout2=nn.Dropout(dropout)\n",
        "    self.layer_norm1=nn.LayerNorm(d_model)\n",
        "    self.multi_head_attn=nn.MultiheadAttention(d_model,num_heads,dropout=dropout,batch_first=True)\n",
        "  def forward(self,tgt):\n",
        "    tgt2,_=self.multi_head_attn(tgt,tgt,tgt)\n",
        "    tgt_residual=tgt+tgt2\n",
        "    tgt_norm=self.layer_norm1(tgt_residual)\n",
        "    tgt_dropout=self.dropout2(tgt_norm)\n",
        "\n",
        "    #pass through the feed forward network\n",
        "    tgt3=self.linear2(self.activation(self.linear1(tgt_dropout)))\n",
        "    tgt_residual2=tgt_dropout+tgt3\n",
        "    tgt_norm2=self.layer_norm2(tgt_residual2)\n",
        "    return tgt_norm2\n",
        "\n"
      ],
      "metadata": {
        "id": "Nlb0NFXpMlFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self,vocab_size, d_model, seq_len, ffd_hidden, num_heads, dropout, num_layers):\n",
        "    super().__init__()\n",
        "    self.embedding = InputEmbedding(d_model, vocab_size)\n",
        "    self.pos_encoding = PositionalEncoding(d_model, dropout, seq_len)\n",
        "\n",
        "    self.Layers=nn.ModuleList([EncoderLayer(d_model,ffd_hidden,num_heads,dropout) for i in range(num_layers)])\n",
        "\n",
        "  def forward(self,x):\n",
        "    x=self.embedding(x)\n",
        "    x=self.pos_encoding(x)\n",
        "    for Layer in self.Layers:\n",
        "      x=Layer(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "mUpcFY_Nmep-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Decoder Code"
      ],
      "metadata": {
        "id": "aYHjOtrvDNDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class transformerdecoderlayer(nn.Module):\n",
        "  def __init__(self,d_model,ffd_hidden,num_heads,dropout):\n",
        "    super().__init__()\n",
        "    self.dropout3=nn.Dropout(dropout)\n",
        "    self.layer_norm3=nn.LayerNorm(d_model)\n",
        "\n",
        "    self.linear3=nn.Linear(ffd_hidden,d_model)\n",
        "    self.activation=nn.ReLU()\n",
        "    self.linear4=nn.Linear(d_model,ffd_hidden)\n",
        "\n",
        "    #Multi_cross_head atention\n",
        "    self.dropout4=nn.Dropout(dropout)\n",
        "    self.layer_norm4=nn.LayerNorm(d_model)\n",
        "    self.multihead_cross_attn=nn.MultiheadAttention(d_model,num_heads,dropout=dropout,batch_first=True)\n",
        "\n",
        "    #masked multi_head self attention\n",
        "    self.dropout5=nn.Dropout(dropout)\n",
        "    self.layer_norm5=nn.LayerNorm(d_model)\n",
        "    self.multihead_self_attn=nn.MultiheadAttention(d_model,num_heads,dropout=dropout,batch_first=True)\n",
        "\n",
        "    #the forward method\n",
        "  def forward(self,tgt,memory,tgt_mask=None,memory_mask=None,tgt_key_padding_mask=None,memory_key_padding_mask=None):\n",
        "    tgt2=self.multihead_self_attn(tgt,tgt,tgt,attn_mask=tgt_mask,key_padding_mask=tgt_key_padding_mask)[0]\n",
        "    tgt2=tgt2+tgt\n",
        "    tgt2_norm=self.layer_norm5(tgt2)\n",
        "    tgt2_dropout=self.dropout5(tgt2_norm)\n",
        "    #cross_attention block\n",
        "    tgt3=self.multihead_cross_attn(tgt2_dropout,memory,memory,attn_mask=memory_mask,key_padding_mask=memory_key_padding_mask)[0]\n",
        "    tgt3=tgt3+tgt2\n",
        "    tgt3_norm=self.layer_norm4(tgt3)\n",
        "    tgt3_dropout=self.dropout4(tgt3_norm)\n",
        "\n",
        "    #feed forward network\n",
        "    tgt4=self.linear4(tgt3_dropout)\n",
        "    tgt4=self.activation(tgt4)\n",
        "    tgt5=self.linear3(tgt4)\n",
        "    tgt5_norm=self.layer_norm3(tgt5)\n",
        "    tgt5_dropout=self.dropout3(tgt5_norm)\n",
        "\n",
        "    tgt5_with_residual=tgt5_norm+tgt3_dropout\n",
        "    tgt5_with_norm=self.layer_norm3(tgt5_with_residual)\n",
        "    tgt_5_with_residual=self.dropout3(tgt5_with_norm)\n",
        "    return tgt_5_with_residual\n",
        "\n"
      ],
      "metadata": {
        "id": "B3qT_xV5DPlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self,vocab_size,d_model,seq_len,ffd_hidden,num_heads,dropout,num_layers):\n",
        "    super().__init__()\n",
        "    self.embedding=InputEmbedding(d_model,vocab_size)\n",
        "    self.pos_encoding=PositionalEncoding(d_model,dropout,seq_len)\n",
        "\n",
        "    #create decoder layers\n",
        "    decoder_layers=[]\n",
        "    for i in range(num_layers):\n",
        "      decoder_layers.append(transformerdecoderlayer(d_model,ffd_hidden,num_heads,dropout))\n",
        "    self.decoder_layers=nn.ModuleList(decoder_layers)\n",
        "\n",
        "    self.linear_layer=nn.Linear(d_model,vocab_size)\n",
        "    self.d_model=d_model\n",
        "  def forward(self,tgt,memory,tgt_mask=None,memory_mask=None,tgt_key_padding_mask=None,memory_key_padding_mask=None):\n",
        "    tgt=self.embedding(tgt)\n",
        "    #optional scaling\n",
        "    tgt=tgt*math.sqrt(self.d_model)\n",
        "    tgt=self.pos_encoding(tgt)\n",
        "\n",
        "    for layer in self.decoder_layers:\n",
        "      tgt=layer(tgt,memory,tgt_mask,memory_mask,tgt_key_padding_mask,memory_key_padding_mask)\n",
        "    raw_scores=self.linear_layer(tgt)\n",
        "    return raw_scores\n",
        " #   probabilities=F.softmax(raw_scores,dim=-1)\n",
        " #   return probabilities"
      ],
      "metadata": {
        "id": "9HkxA25iJS-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9EkvORH1iQuy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#for encoder\n",
        "batch_size=32\n",
        "d_model=512\n",
        "seq_len=100\n",
        "vocab_size=1000\n",
        "x = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
        "y=torch.randint(0,vocab_size,(batch_size,seq_len))\n",
        "encoder = Encoder( vocab_size, d_model, seq_len, ffd_hidden=2048, num_heads=8, dropout=0.1, num_layers=6)\n",
        "out = encoder(x)"
      ],
      "metadata": {
        "id": "5bMH1KD080Im",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "d01115d6-873b-4665-cc8c-61112e7897c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Encoder' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1123428026.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mffd_hidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Encoder' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#for decoder\n",
        "vocab_size=100\n",
        "d_model=4#should be 512\n",
        "max_len=3\n",
        "num_decoder_layers=1\n",
        "dim_ffd=6#generally 2048\n",
        "dropout=0.1\n",
        "n_heads=1\n",
        "num_layers=2#typically six\n",
        "\n",
        "#create our decoder model\n",
        "model=Decoder(vocab_size, d_model, max_len, dim_ffd, n_heads, dropout,num_layers=1)\n",
        "\n",
        "#create dummy inputs\n",
        "y=torch.randint(0, vocab_size, (2, max_len))\n",
        "x=torch.randn(2,max_len,d_model)\n",
        "output=model(y,x)\n",
        "\n",
        "#criterion\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "output=model(y,x)\n",
        "loss=criterion(output.view(-1,vocab_size),y.view(-1))\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcL0yHR9jeD_",
        "outputId": "b74e6757-675f-498b-de3d-80a128306e02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(4.4412, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    output = model(y, x)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = criterion(output.view(-1, vocab_size), y.view(-1))\n",
        "\n",
        "    # Backward pass + optimize\n",
        "    optimizer.zero_grad()   # clear old gradients\n",
        "    loss.backward()         # backprop\n",
        "    optimizer.step()        # update parameters\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "EDpLOh2TDKkI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6036e71-da7c-4f11-c7c5-2fc866bd45ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 4.4004\n",
            "Epoch [2/10], Loss: 4.5425\n",
            "Epoch [3/10], Loss: 4.5300\n",
            "Epoch [4/10], Loss: 4.3042\n",
            "Epoch [5/10], Loss: 4.4423\n",
            "Epoch [6/10], Loss: 4.6995\n",
            "Epoch [7/10], Loss: 4.4409\n",
            "Epoch [8/10], Loss: 4.4027\n",
            "Epoch [9/10], Loss: 4.4102\n",
            "Epoch [10/10], Loss: 4.4401\n"
          ]
        }
      ]
    }
  ]
}